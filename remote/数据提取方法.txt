1.基础知识
    数据分类:
        -非结构化数据:html等
            处理方法:正则表达式,xpath

        -结构化数据:json,xml等
            处理方法:转化为python数据类型


2.json知识
    将json数据转化为python内建数据非常简单,所以在爬虫中,如果能够找到返回json数据的url,就尽可能使用这种url.
    json是一种轻量级的数据交换格式.适用于进行数据交互的场景,比如网站前台与后台的交互.

    哪里可以找到返回json的url?
        -使用chorme切换到手机界面
        -抓包手机app的软件

    json和python类型的相互转换
        -json.loads() 将json字符串转换为python字典
        -json.dumps() 将python字典类型转换为json字符串


3.正则表达式
    事先定义好的特定的字符,及这些特定字符的组合,组成一个"规则字符串",这个"规则字符串"用来表达对字符串的一种过滤逻辑

    常用的正则表达方法:
        re.compile(编译)
        pattern.match(从头找一个)
        pattern.search(找一个)
        pattern.findall(找所有)
        pattern.sub(替代)


4.xml了解

    xpath是一门在HTML/XML文档中查找信息的语言,可以用来在HTML\XML文档中对元素和属性进行遍历

    XML:可扩展标记语言,被设计为传输和存储数据,其焦点是数据的内容
    HTML:超文本标记语言,显示(展示)数据以及如何更好的显示数据

    namenode选取此节点的所有节点
    /从根节点选取
    //从匹配选择的当前节点选文本的节点,而不考虑他们的位置
    .选取当前节点
    ..选取当前节点的父节点
    @选取属性


    使用chorme插件选择标签的时候,选中时,选中的标签会添加属性class="xh-highlight"

    使用xpath helper或者是chorme中的copy path都是从element中提取数据,但是爬虫获取的是url对应的响应,往往和elements不一样

    关键方法:
        1.获取文本
            -a/text()获取节点下的文本
            -a//text()获取a下所有标签的文本
            -//a[text()="下一页>"]选择文本为下一页三个字的a标签
        2.@符号
            a/@href获取a标签的所有href
            //ul[@id='detail-list']
        3.//
            在xpath开始的时候表示从当前html中任意位置开始
            li//a表示的是li下任何一个标签


5.lxml
    lxml是一款高性能的Python HTML\XML解析器,利用xpath,来快速的定位元素以及获取节点信息
    from lxml import etree
    利用etree.HTML,将字符串转化为Elements对象
    Element对象具有xpath方法
    html = etree.HTML(text)


    lxml使用注意点:
    1.lxml能够修正html代码,但可能会改错了
        -使用etree.tostring观察修改之后的html的样子,根据修改之后的html字符串写xpath
    2.提取页面数据的思路
        -先分组,取到一个包含分组标签的列表
        -遍历,取其中每一组进行数据的提取,不会造成数据的错乱(有的组中有值为空,刻意设置为None)
    3.lxml能够接收bytes和str字符串


6.实现爬虫搞得套路:
    -准备url
        -准备start_url
            -url地址规律不明显,总数不确定
            -通过代码提取下一页的url
                -xpath
                -寻找url地址,部分参数在当前的响应中(比如当前页码数和总的页码数在当前的响应中)
        -准备url_list
            -页码总数明确
            -url地址规律明显

    -发送请求,获取响应
        -添加随机的User-Agent,反反爬虫
        -添加随机的代理ip,反反爬虫
        -在对方判断出我们是爬虫后,应该添加更多的headers字段,包括cookie
        -cookie这一块的处理,我们可以使用session来处理
        -准备一堆能用的cookie,组成cookie池
            -如果不登录
                -准备刚开始能够成功请求对方网站的cookie,即接收对方网站设置在response的cookie
                -下一次使用的时候,使用之前的列表中cookie请求
            -如果登录
                -准备多个账号
                -使用程序获取每个账号的cookie
                -之后请求登陆之后才能访问网站随机的选择cookie

    -提取数据
        -确定数据的位置
            -如果数据在当前的url地址中
                -提取的是列表页的数
                    -直接请求列表页的url地址,不用进入详情页
                -提取的是详情页的数据
                    -1.确定url
                    -2.发送请求
                    -3.提取数据
                    -4.返回
            -如果数据不在当前的url地址中
                -在其他响应中,寻找数据的位置
                    1.从network中从上往下找
                    2.使用chrome中过滤条件多选,选择除了js,css,img之外的按钮
                    3.使用chrome的search all file,搜索数字和英文
        -数据的提取
            -xpath,从html中提取整块的数据,先分组,每一组在提取数据
            -re,提取max_time,price,html中的json字符串
            -json

    -保存
        -保存到本地,txt,json,csv
        -保存到数据库

7.爬虫的建议
    -尽量减少请求的次数
        -1.能抓取列表页就不抓取详情页
        -2.保存获取到的html页面,供查错与重复请求使用
    -关注网站所有类型的页面
        -1.wap页面,触屏版页面
        -2.H5页面
        -3.APP
    -多伪装
        -1.动态的UA
        -2.代理ip
        -3.不使用cookie
    -利用多线程,分布式
        -在不被ban的情况下尽可能的提高速度

8.动态HTMl技术了解
    -JS:网络上最常用的脚本语言,可以搜集用户的跟踪数据,不需要重载页面直接提交表单,在页面嵌入多媒体文件,甚至运行网页游戏
    -jQuery:是一个快速简洁的js框架,封装了js常用的代码功能
    -Ajax:可以是网页实现异步更新,可以在不重新加载整个页面的情况下,对网页的某部分进行更新

    对搜索引擎不友好,对爬虫也不友好

9.Selenium和PhantomJS(实现动态爬虫)
    Selenium:是一个web自动化测试工具,最初是为网站自动化测试而开发的,Selenium可以直接运行在浏览器上,它支持所有主流浏览器(包括PhantomJS这些无界面浏览器),
    可以接受指令,让浏览器自动加载页面,获取需要的数据,甚至页面截屏

    PhantomJS:是一个基于webkit的"无界面浏览器",它会把网站加载到内存并执行页面上的JavaScript(目前已经被弃用)

10.打码平台
    -云打码

    验证码的识别:
        -url不变,验证码不变
            -请求验证码的地址,获得响应,识别
        -url不变,验证码变
            -思路:对方服务器返回验证码的时候,会把每个用户的信息和验证码进行一个对应,之后在用户发送post请求的时候,
                会对比post请求中的验证码和当前用户存储在服务器段的邀请你正码是否相等
            -1.实例化session
            -2.使用session请求登录界面,获取验证码地址
            -3.使用session请求验证码,识别
            -4.使用session发送post请求
    使用selenium登录,遇到验证码
        -url不变,验证码不变,同上
        -url不变,验证码会变
            -1.selenium请求登录页面
            -2.获取登录页面中driver中的cookie,交给requests模块发送验证码请求,识别
            -3.输入验证码,点击登录





